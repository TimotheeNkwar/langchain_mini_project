# ğŸ¯ LangChain Mini Project - Q&A Assistant

A simple and clean Python project demonstrating how **LangChain** works with a practical Q&A assistant that answers questions based on uploaded documents.

> ğŸš€ **Get started in 5 minutes!** | ğŸ“š **Learn by example** | ğŸ”§ **No API keys required**

## âœ¨ Features at a Glance

- âœ… **Zero API Dependencies** - Uses local embeddings and LLM
- âœ… **Production-Ready Architecture** - Modern LCEL (LangChain Expression Language) syntax
- âœ… **Comprehensive Knowledge Base** - 1000+ lines of LangChain documentation included
- âœ… **Easy to Understand** - Clean, well-documented code with step-by-step workflow
- âœ… **Fully Extensible** - Simple structure makes it easy to add features
- âœ… **Educational Value** - Includes 25 test questions with expert-level scoring rubric
- âœ… **Fast** - FAISS vector database for near-instant retrieval

## ğŸ“š Project Overview

This project implements a **Question & Answer system** that:

1. Loads a text document (`data.txt`) containing your knowledge base
2. Splits it into manageable chunks (500 chars with 50 char overlap)
3. Creates embeddings (numerical representations) of the text
4. Stores embeddings in a FAISS vector database for fast similarity search
5. Answers user questions by retrieving relevant context and using an LLM to generate responses

It's a practical implementation of the **Retrieval-Augmented Generation (RAG)** pattern, perfect for learning how modern NLP systems work.

---

## âš¡ Quick Start (5 Minutes)

### 1. Install Dependencies

```bash
cd langchain_mini_project
uv pip install -r requirements.txt
```

### 2. Run the Q&A System

```bash
python app.py
```

### 3. Ask Questions

```
â“ Your question: What is LangChain?
ğŸ¤– Answer: LangChain is a framework for developing applications powered by language models. It enables applications that are: data-aware (connect a language model to other sources of data), agentic (allow a language model to interact with its environment), and modular...

â“ Your question: What are the main components?
ğŸ¤– Answer: The main components include: LLMs (Language Models), Prompts, Chains, Memory, Retrievers, Document Loaders, Text Splitters, Embeddings, and Vector Stores. Each serves a specific purpose in the knowledge pipeline...

â“ Your question: exit
[Program exits]
```

**That's it!** You now have a working Q&A system. ğŸ‰

---

## ğŸ› ï¸ How It Works

### Architecture Flow

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA PIPELINE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  data/data.txt â†’ TextLoader â†’ RecursiveCharacterSplitter   â”‚
â”‚      â†“              â†“              â†“                         â”‚
â”‚   Raw Text    Split Chunks   Chunk Management               â”‚
â”‚                      â†“                                       â”‚
â”‚              HuggingFaceEmbeddings                           â”‚
â”‚                      â†“                                       â”‚
â”‚              FAISS Vector Store                              â”‚
â”‚                      â†“                                       â”‚
â”‚         (Ready for similarity search)                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†‘
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RETRIEVAL PIPELINE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  User Question â†’ Embed Question â†’ Search Vector Store      â”‚
â”‚        â†“               â†“               â†“                     â”‚
â”‚   Input Text   Numerical Vector  Find K=4 Similar Chunks    â”‚
â”‚                                      â†“                       â”‚
â”‚                          Retrieve Context from K Chunks      â”‚
â”‚                                      â†“                       â”‚
â”‚                        Format Context + Question             â”‚
â”‚                                      â†“                       â”‚
â”‚                   OllamaLLM (Generate Answer)                â”‚
â”‚                                      â†“                       â”‚
â”‚                          Return Final Answer                 â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Step-by-Step Process

#### 1ï¸âƒ£ **Document Loading** (`embeddings.py`)

- Reads `data/data.txt` using `TextLoader`
- Loads full document content into memory as raw text
- Validates file exists and contains expected content

#### 2ï¸âƒ£ **Text Splitting** (`embeddings.py`)

- Splits document into overlapping chunks:
  - **Chunk size**: 500 characters per chunk
  - **Overlap**: 50 characters between chunks
  - **Why**: Maintains context at chunk boundaries, prevents losing meaning
- Uses `RecursiveCharacterTextSplitter` (respects paragraph breaks first, then sentences, then words)

**Example:**

```text
Original chunk ends with: "...Vector stores like FAISS" 
Next chunk starts with:   "...FAISS and Pinecone"
(50 char overlap preserves connection)
```

#### 3ï¸âƒ£ **Embeddings Creation** (`embeddings.py`)

- Converts each text chunk to a numerical vector (384 dimensions)
- Uses `HuggingFaceEmbeddings` with `sentence-transformers/all-MiniLM-L6-v2`
- All processing is **local** - no API calls, no cloud dependency
- First run downloads model (~50MB)

**Why HuggingFace?**

- âœ… Free and open-source
- âœ… Fast inference (CPU-based)
- âœ… Privacy-preserving (no data leaves machine)
- âœ… 384-dimensional embeddings (good quality for cost)

#### 4ï¸âƒ£ **Vector Store Creation** (`embeddings.py`)

- Stores all embeddings in FAISS (Facebook AI Similarity Search)
- Creates searchable index of document chunks
- Enables fast nearest-neighbor search
- Runs entirely in-memory (perfect for this project size)

#### 5ï¸âƒ£ **Retrieval Chain** (`chains.py`)

- When a question is asked:
  1. Converts question to an embedding
  2. Searches FAISS for 4 most similar chunks
  3. Retrieves full text of those chunks
  4. Formats them as context

- Uses **LCEL (LangChain Expression Language)**:

```python
qa_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
)
```

#### 6ï¸âƒ£ **LLM Processing** (`chains.py`, `llm.py`, `app.py`)

- Sends formatted prompt with context to `OllamaLLM`
- LLM reads context chunks and generates answer
- Uses `temperature=0.9` for balanced creativity

**Flow:**

- Question + Context â†’ LLM â†’ Contextual Answer
- Model: `llama-2-7b-chat` (7 billion parameters)
- Response time: ~1-3 seconds on CPU

---

## ğŸ“ Project Structure

```text
langchain_mini_project/
â”œâ”€â”€ README.md                    # ğŸ“– This file
â”œâ”€â”€ app.py                       # ğŸš€ Main entry point
â”œâ”€â”€ llm.py                       # ğŸ§  LLM initialization
â”œâ”€â”€ embeddings.py                # ğŸ”— Document & vector store setup
â”œâ”€â”€ chains.py                    # â›“ï¸  Retrieval chain (LCEL syntax)
â”œâ”€â”€ config.py                    # âš™ï¸  Configuration & env loading
â”œâ”€â”€ requirements.txt             # ğŸ“¦ Python dependencies
â”œâ”€â”€ data/
â”‚   â””â”€â”€ data.txt                 # ğŸ“š Knowledge base (2500+ lines)
â””â”€â”€ testing_questions.txt        # â“ 25 test questions with rubric
```

### Detailed File Descriptions

| File | Purpose | Key Features |
|------|---------|--------------|
| **app.py** | Main CLI interface | Interactive loop, error handling, graceful exit |
| **llm.py** | LLM initialization | Loads `llama-2-7b-chat`, sets temperature=0.9 |
| **embeddings.py** | Document pipeline | Loading â†’ Splitting â†’ Embedding â†’ Vector Store |
| **chains.py** | LCEL retrieval chain | Pipe syntax, context formatter, prompt template |
| **config.py** | Configuration management | Loads .env variables for flexibility |
| **data/data.txt** | Knowledge base | Comprehensive LangChain documentation |
| **testing_questions.txt** | Test suite | 25 questions, 7 categories, scoring rubric |

### Code Overview

#### **app.py** - The Entry Point

```python
# Interactive Q&A loop
while True:
    question = input("â“ Your question: ")
    if question.lower() == "exit":
        break
    
    # Invoke the chain with the question
    response = chain.invoke({"question": question})
    print(f"ğŸ¤– Answer: {response}\n")
```

#### **llm.py** - LLM Configuration

```python
from langchain_ollama import OllamaLLM

llm = OllamaLLM(
    model="llama-2-7b-chat",    # Local model
    temperature=0.9,             # 0=factual, 1=creative
)
```

#### **embeddings.py** - Document Processing Pipeline

```python
# 1. Load document
loader = TextLoader("data/data.txt")
documents = loader.load()

# 2. Split into chunks
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
)
chunks = splitter.split_documents(documents)

# 3. Create embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# 4. Build vector store
vector_store = FAISS.from_documents(
    chunks, 
    embeddings
)
```

#### **chains.py** - LCEL Retrieval Chain

```python
from langchain_core.runnables import RunnablePassthrough

# Modern LangChain syntax (LCEL)
qa_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt_template
    | llm
    | StrOutputParser()  # Parse LLM output to string
)

# Usage:
response = qa_chain.invoke({"question": "What is LangChain?"})
```

---

## ğŸš€ Installation

### Prerequisites

- Python 3.10+
- `uv` package manager (or `pip`)

### Setup

1. **Clone or navigate to the project**

```bash
cd langchain_mini_project
```

1. **Install dependencies with uv**

```bash
uv pip install -r requirements.txt
```

Or with pip:

```bash
pip install -r requirements.txt
```

1. **Download the embedding model (one-time)**
The first run will download the `sentence-transformers/all-MiniLM-L6-v2` model (~50MB).

---

## ğŸ’» Usage

### Run the Q&A Assistant

```bash
python app.py
```

### Example Questions

```text
â“ Your question: What is LangChain?
ğŸ¤– Answer: LangChain is a powerful framework for developing applications...

â“ Your question: What are the key components?
ğŸ¤– Answer: The key components include LLMs, Prompts, Chains, Memory...

â“ Your question: exit
[Program exits]
```

---

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file if you need API keys (currently not needed):

```bash
OPENAI_API_KEY=your_key_here  # Not required for this project
```

### Model Parameters (in `llm.py`)

- `model="llama-2-7b-chat"` - The language model to use
- `temperature=0.9` - Creativity level (0=factual, 1=creative)

### Chunk Settings (in `embeddings.py`)

- `chunk_size=500` - Size of text chunks
- `chunk_overlap=50` - Overlap between chunks

---

## ğŸ“¦ Dependencies

| Package | Purpose |
|---------|---------|
| `langchain` | Core framework |
| `langchain-community` | Additional integrations |
| `langchain-ollama` | Local LLM integration |
| `langchain-text-splitters` | Text chunking |
| `sentence-transformers` | Embedding model |
| `faiss-cpu` | Vector database |
| `python-dotenv` | Environment variables |

---

## ğŸ“ Learning Resources

This project teaches:

- **Document Processing**: Loading and splitting text
- **Embeddings**: Converting text to vectors
- **Vector Databases**: Storing and searching embeddings
- **Retrieval Chains**: Finding relevant context
- **LLM Integration**: Using local language models
- **LCEL**: LangChain Expression Language syntax

---

## ğŸ”„ How the Chain Works (LCEL Syntax)

```python
qa_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
)
```

This creates a pipeline where:

1. The retriever gets relevant context chunks
2. The question passes through unchanged
3. Both are formatted into a prompt
4. The LLM processes and generates an answer

---

## ğŸŒ No API Keys Required

This project is completely local:

- âœ… Uses local embedding model (HuggingFace)
- âœ… Uses local LLM (Ollama/Llama2)
- âœ… No OpenAI API calls
- âœ… Runs entirely offline (after first model download)

---

## ğŸ“ Notes

- The first run will download the embedding model (~50MB)
- Responses depend on the quality of documents in `data/data.txt`
- You can replace `data.txt` with any text file you want to analyze
- The vector store is created in-memory each time (not persisted)

---

## ğŸ¤ Common Issues & Solutions

| Issue                   | Solution                                           |
| ----------------------- | -------------------------------------------------- |
| `ModuleNotFoundError`   | Run `uv pip install -r requirements.txt`           |
| Slow first run          | First request downloads embedding model, be patient |
| No response from LLM    | Ensure Ollama is running (if using Ollama locally) |
| Type warnings           | Disable Pylance type checking in VS Code settings  |

---

## ğŸ“š Further Learning

To extend this project, you could:

- Add memory (remember previous conversations)
- Support multiple documents
- Add a web interface (Flask/FastAPI)
- Use different LLMs (GPT, Claude, etc.)
- Persist the vector store to disk
- Add document chat features

---

## ğŸ“„ License

This project is provided as a learning resource.

Happy coding! ğŸš€
