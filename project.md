# ğŸ¯ Miniâ€‘projet simple : comprendre comment LangChain fonctionne

Objectif : comprendre **le fonctionnement de LangChain** avec un **petit projet clair**, une **architecture propre** et des **fichiers sÃ©parÃ©s**.

---

## ğŸ§  IdÃ©e du projet

Un **assistant Q&A sur un fichier texte** :

* Tu donnes un fichier `data.txt`
* LangChain le dÃ©coupe
* CrÃ©e des embeddings
* Les stocke dans un vector store
* Un LLM rÃ©pond aux questions en se basant sur le texte

Câ€™est le workflow LangChain classique ğŸ”¥

---

## ğŸ—‚ï¸ Structure du projet

```
langchain_mini_project/
â”‚
â”œâ”€â”€ app.py                # Point dâ€™entrÃ©e
â”œâ”€â”€ config.py             # Configuration globale
â”œâ”€â”€ llm.py                # Chargement du LLM
â”œâ”€â”€ embeddings.py         # Embeddings + Vector Store
â”œâ”€â”€ chain.py              # ChaÃ®ne LangChain
â”œâ”€â”€ data/
â”‚   â””â”€â”€ data.txt          # Texte source
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ 1. requirements.txt

```txt
langchain
langchain-community
langchain-openai
faiss-cpu
python-dotenv
```

---

## âš™ï¸ 2. config.py

Centralise la config (bonne pratique)

```python
import os
from dotenv import load_dotenv

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
```

---

## ğŸ¤– 3. llm.py â€“ ModÃ¨le local avec Ollama

On utilise un **LLM 100% local** via Ollama (ex: Mistral, Llama3).

```python
from langchain_community.llms import Ollama


def load_llm():
    return Ollama(
        model="mistral",
        temperature=0
    )
```

â¡ï¸ Avantages :

* Pas dâ€™API Key
* DonnÃ©es locales
* Gratuit

---

## ğŸ§¬ 4. embeddings.py â€“ Texte â†’ vecteurs

```python
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS


def create_vectorstore():
    loader = TextLoader("data/data.txt")
    documents = loader.load()

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50
    )
    chunks = splitter.split_documents(documents)

    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(chunks, embeddings)

    return vectorstore
```

â¡ï¸ Ici tu vois le cÅ“ur **RAG** :
Texte â†’ chunks â†’ embeddings â†’ base vectorielle

---

## ğŸ”— 5. chain.py â€“ La magie LangChain

```python
from langchain.chains import RetrievalQA


def create_chain(llm, vectorstore):
    retriever = vectorstore.as_retriever()

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever
    )

    return qa_chain
```

â¡ï¸ LangChain connecte :
**Question â†’ recherche vectorielle â†’ contexte â†’ LLM â†’ rÃ©ponse**

---

## â–¶ï¸ 6. app.py â€“ Point dâ€™entrÃ©e

```python
from llm import load_llm
from embeddings import create_vectorstore
from chain import create_chain


def main():
    llm = load_llm()
    vectorstore = create_vectorstore()
    qa_chain = create_chain(llm, vectorstore)

    while True:
        question = input("â“ Ta question : ")
        if question.lower() == "exit":
            break

        answer = qa_chain.run(question)
        print("\nğŸ¤– RÃ©ponse :", answer, "\n")


if __name__ == "__main__":
    main()
```

---

## ğŸ§ª Exemple de data/data.txt

```txt
LangChain est une bibliothÃ¨que Python permettant de construire des applications basÃ©es sur des modÃ¨les de langage.
Elle facilite la crÃ©ation de chaÃ®nes combinant LLM, outils, mÃ©moire et bases vectorielles.
```

---

## ğŸ” RÃ©sumÃ© mental (super important)

```
Texte
  â†“
DÃ©coupage
  â†“
Embeddings
  â†“
Vector Store (FAISS)
  â†“
Retriever
  â†“
LLM
  â†“
RÃ©ponse
```

---

## ğŸš€ Ã‰tapes suivantes (si tu veux aller plus loin)

* Ajouter **mÃ©moire de conversation**
* Passer Ã  **Streamlit**
* Utiliser un **LLM local (Ollama)**
* Sauvegarder FAISS sur disque

Si tu veux, je peux te faire :
ğŸ‘‰ la **version Streamlit**
ğŸ‘‰ une **version 100% openâ€‘source**
ğŸ‘‰ ou tâ€™expliquer **chaque ligne lentement**
